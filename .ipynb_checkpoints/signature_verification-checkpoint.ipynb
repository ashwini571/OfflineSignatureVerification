{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_eJJI2JYBoE"
   },
   "source": [
    "### Importing libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IG0O5QmPYBoG"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import ndimage\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Colab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlWX3QtSx3Hr"
   },
   "source": [
    "### Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F02tqO7zfx9h",
    "outputId": "8a9f987d-08d2-4b2b-f968-672508ba0bc1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-yYEl88YBoN"
   },
   "source": [
    "Path to store weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdXjGHtDYBoO",
    "outputId": "ae4d0ec3-9424-43af-bac6-b0af0fb1db48"
   },
   "outputs": [],
   "source": [
    "path_weights='/content/drive/My Drive/Colab Notebooks/weights3/'\n",
    "dir_list = os.listdir(path_weights) \n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgOA84IvYBoW"
   },
   "source": [
    "Path to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcpR9d5_YBoX"
   },
   "outputs": [],
   "source": [
    "path_dataset = r\"/content/drive/My Drive/Colab Notebooks/datasets/cedar_dataset\"\n",
    "dir_list = next(os.walk(path_dataset))[1]\n",
    "dir_list.sort()\n",
    "dir_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['full_forg', 'full_org']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_weights='Model/Weights/'\n",
    "path_dataset = 'Datasets/cedar_dataset'\n",
    "dir_list = next(os.walk(path_dataset))[1]\n",
    "dir_list.sort()\n",
    "dir_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLCjdH2nDfU0"
   },
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqUF8Ws7Dd9g"
   },
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    clean = remove_background(image)\n",
    "    roi = extract_signature(clean)\n",
    "    th,res= cv2.threshold(roi, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # r = ndimage.rotate(res, -(90-math.degrees(math.atan(res.shape[1]/res.shape[0]))))\n",
    "    # angle = 90-math.degrees(math.atan(res.shape[1]/res.shape[0]))\n",
    "    # if angle > 20:\n",
    "    #     res = rotate_image(res, -(angle if angle>20 else 0))\n",
    "   \n",
    "    return res\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "  rot_mat = cv2.getRotationMatrix2D((image.shape[0]/2+40,image.shape[1]/2+40), angle, 1.0)\n",
    "  result = cv2.warpAffine(image, rot_mat, (int(math.sqrt(image.shape[0]*image.shape[0]+image.shape[1]*image.shape[1])),int(image.shape[1])), flags=cv2.INTER_LINEAR,borderValue=(255,255,255))\n",
    "  return result\n",
    "\n",
    "\n",
    "\n",
    "def extract_signature(image):\n",
    "    result = image.copy()\n",
    "    img = image.copy()\n",
    "    ret, mask = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "    # opening = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    cnts,hi = cv2.findContours(closing.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    boxes = []\n",
    "    for c in cnts:\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        boxes.append([x,y, x+w,y+h])\n",
    "    \n",
    "    boxes = np.asarray(boxes)\n",
    "    left = np.min(boxes[:,0])\n",
    "    top = np.min(boxes[:,1])\n",
    "    right = np.max(boxes[:,2])\n",
    "    bottom = np.max(boxes[:,3])\n",
    "    \n",
    "    result[closing==0] = (255)\n",
    "    ROI = result[top:bottom, left:right].copy()\n",
    "    cv2.rectangle(result, (left,top), (right,bottom), (36, 255, 12), 2)\n",
    "    \n",
    "    # cv2.imshow('result', result)\n",
    "    # cv2.imshow('ROI', ROI)\n",
    "    # cv2.imshow('Mask', mask)\n",
    "    # cv2.imshow('closing',closing)\n",
    "\n",
    "    cv2.waitKey()\n",
    "    \n",
    "    return ROI\n",
    "\n",
    "\n",
    "def remove_background(img):\n",
    "        \"\"\" Remove noise using OTSU's method.\n",
    "\n",
    "        :param img: The image to be processed\n",
    "        :return: The normalized image\n",
    "        \"\"\"\n",
    "        img = img.astype(np.uint8)\n",
    "        # Binarize the image using OTSU's algorithm. This is used to find the center\n",
    "        # of mass of the image, and find the threshold to remove background noise\n",
    "    \n",
    "        threshold, _ = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        # Remove noise - anything higher than the threshold. Note that the image is still grayscale\n",
    "        img[img > threshold] = 255\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset directory list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BHSig260(path):\n",
    "    dir_list = next(os.walk(path))[1]\n",
    "    dir_list.sort()    \n",
    "    original = []\n",
    "    forged = []\n",
    "    for directory in dir_list:\n",
    "        images = os.listdir(path+directory)\n",
    "        images.sort()\n",
    "        images = [path+directory+'/'+x for x in images]\n",
    "        forged.append(images[:30]) # First 30 signatures in each folder are forged\n",
    "        original.append(images[30:]) # Next 24 signatures are genuine\n",
    "    \n",
    "    return original, forged\n",
    "\n",
    "\n",
    "def Cedar(path):\n",
    "    dir_original_list = os.listdir(path + '/full_org')\n",
    "    dir_original_list = [path+'/full_org'+'/'+x for x in dir_original_list]\n",
    "    dir_forged_list = os.listdir(path + '/full_forg')\n",
    "    dir_forged_list = [path+'/full_forg'+'/'+x for x in dir_forged_list]\n",
    "    \n",
    "    original = []\n",
    "    forged = [] \n",
    "    i=0\n",
    "    while i < (len(dir_original_list)):\n",
    "        original.append(dir_original_list[i:i+12])\n",
    "        forged.append(dir_forged_list[i:i+12])\n",
    "        i = i+12\n",
    "        \n",
    "    return original, forged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_list,forged_list = Cedar(path_dataset)\n",
    "len(forged_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4__7Lan9y865",
    "outputId": "6e80b514-54a5-433b-c09e-12e793bd7029"
   },
   "outputs": [],
   "source": [
    "# prints 1 1 if length of each group is same\n",
    "def sanity_check():\n",
    "    orig_lengths = [len(x) for x in original_list]\n",
    "    forg_lengths = [len(x) for x in forged_list]\n",
    "    f_original=0\n",
    "    for i in range(0,len(orig_lengths)):\n",
    "        for j in range(i+1,len(orig_lengths)):\n",
    "                   if orig_lengths[i]==orig_lengths[j]:\n",
    "                       f_original=1\n",
    "    f_forged=0\n",
    "    for i in range(0,len(forg_lengths)):\n",
    "        for j in range(i+1,len(forg_lengths)):\n",
    "                   if forg_lengths[i]==forg_lengths[j]:\n",
    "                       f_forged=1\n",
    "    print(f_original)\n",
    "    print(f_forged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9iV9VZ0zQNP",
    "outputId": "d6c788d1-380e-4636-9bf9-731bbc75fbce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aum6UQRMYBpR"
   },
   "source": [
    "## Dataset Split - train-validation-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TKCZmoSSYBpR"
   },
   "outputs": [],
   "source": [
    "orig_train, orig_val, orig_test = original_list[:70], original_list[70:90], original_list[90:]\n",
    "forg_train, forg_val, forg_test = forged_list[:70], forged_list[70:90], forged_list[90:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgtVxf4qYBqO"
   },
   "source": [
    "## Generating batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1TPrDIVOYBpd"
   },
   "outputs": [],
   "source": [
    "img_h, img_w = 155, 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sKIs3slFYBqU"
   },
   "outputs": [],
   "source": [
    "def generate_batch(orig_groups, forg_groups, batch_size = 32):\n",
    "    while True:\n",
    "        orig_pairs = []\n",
    "        forg_pairs = []\n",
    "        gen_gen_labels = []\n",
    "        gen_for_labels = []\n",
    "        all_pairs = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Here we create pairs of Genuine-Genuine image names and Genuine-Forged image names\n",
    "        # For every person we have 12 genuine signatures in a group, hence we have \n",
    "        # 12 choose 2 = 66 Genuine-Genuine image pairs for one person.\n",
    "        # To make Genuine-Forged pairs, we pair every Genuine signature of a person\n",
    "        # with 6 randomly sampled Forged signatures of the same person.\n",
    "\n",
    "        \n",
    "        for orig, forg in zip(orig_groups, forg_groups):\n",
    "            orig_pairs.extend(list(itertools.combinations(orig, 2)))\n",
    "            for i in range(len(forg)):\n",
    "                forg_pairs.extend(list(itertools.product(orig[i:i+1], random.sample(forg, 6)))) #6 for 6 samples random\n",
    "        \n",
    "        # Label for Genuine-Genuine pairs is 1\n",
    "        # Label for Genuine-Forged pairs is 0\n",
    "        gen_gen_labels = [1]*len(orig_pairs)\n",
    "        gen_for_labels = [0]*len(forg_pairs)\n",
    "        \n",
    "        # Concatenate all the pairs together along with their labels and shuffle them\n",
    "        all_pairs = orig_pairs + forg_pairs\n",
    "        all_labels = gen_gen_labels + gen_for_labels\n",
    "        del orig_pairs, forg_pairs, gen_gen_labels, gen_for_labels\n",
    "        all_pairs, all_labels = shuffle(all_pairs, all_labels)\n",
    "\n",
    "        k = 0\n",
    "        pairs=[np.zeros((batch_size, img_h, img_w, 1)) for i in range(2)]\n",
    "        targets=np.zeros((batch_size,))\n",
    "        for ix, pair in enumerate(all_pairs):\n",
    "            img1 = cv2.imread(pair[0], 0)\n",
    "            img2 = cv2.imread(pair[1], 0)\n",
    "            # img1 = preprocess(img1)\n",
    "            # img2 = preprocess(img2)\n",
    "            img1 = cv2.resize(img1, (img_w, img_h))\n",
    "            img2 = cv2.resize(img2, (img_w, img_h))\n",
    "            img1 = np.array(img1, dtype = np.float64)\n",
    "            img2 = np.array(img2, dtype = np.float64)\n",
    "            img1 /= 255\n",
    "            img2 /= 255\n",
    "            img1 = img1[..., np.newaxis]\n",
    "            img2 = img2[..., np.newaxis]\n",
    "            pairs[0][k, :, :, :] = img1\n",
    "            pairs[1][k, :, :, :] = img2\n",
    "            targets[k] = all_labels[ix]\n",
    "            k += 1\n",
    "            if k == batch_size:\n",
    "                yield pairs, targets\n",
    "                k = 0\n",
    "                pairs=[np.zeros((batch_size, img_h, img_w, 1)) for i in range(2)]\n",
    "                targets=np.zeros((batch_size,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fNvUu2iC7fX"
   },
   "source": [
    "## Similairity Metric and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aYl8QtI9YBqP"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# cosine_similarity([[1, 0, -1]], [[-1,-1, 0]])\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    '''Compute Euclidean Distance between two vectors'''\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "U4p1B0c5YBqS"
   },
   "outputs": [],
   "source": [
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ski2oOjIYBqZ"
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg2oDXUuDApK"
   },
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MjulEXLaYBqe"
   },
   "outputs": [],
   "source": [
    "def network_architecture(input_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(96, kernel_size=(11, 11), activation='relu', name='conv1_1', strides=4, input_shape= input_shape, \n",
    "                        kernel_initializer='glorot_uniform', data_format='channels_last'))\n",
    "\n",
    "    model.add(BatchNormalization(epsilon=1e-06, axis=1, momentum=0.9))\n",
    "    model.add(MaxPooling2D((3,3), strides=(2, 2)))    \n",
    "    model.add(ZeroPadding2D((2, 2), data_format='channels_last'))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=(5, 5), activation='relu', name='conv2_1', strides=1, kernel_initializer='glorot_uniform', data_format='channels_last'))\n",
    "    model.add(BatchNormalization(epsilon=1e-06, axis=1, momentum=0.9))\n",
    "    model.add(MaxPooling2D((3,3), strides=(2, 2)))\n",
    "    model.add(Dropout(0.3))# added extra\n",
    "    model.add(ZeroPadding2D((1, 1), data_format='channels_last'))\n",
    "    \n",
    "    model.add(Conv2D(384, kernel_size=(3, 3), activation='relu', name='conv3_1', strides=1, kernel_initializer='glorot_uniform', data_format='channels_last'))\n",
    "    model.add(ZeroPadding2D((1, 1), data_format='channels_last'))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', name='conv3_2', strides=1, kernel_initializer='glorot_uniform', data_format='channels_last'))    \n",
    "    model.add(MaxPooling2D((3,3), strides=(2, 2)))\n",
    "    model.add(Dropout(0.3))# added extra\n",
    "    \n",
    "    model.add(Flatten(name='flatten'))\n",
    "\n",
    "    model.add(Dense(1024, bias_regularizer=l2(0.0005), activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(128, bias_regularizer=l2(0.0005), activation='relu', kernel_initializer='glorot_uniform')) # softmax changed to relu\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "g0igJpf_YBqh"
   },
   "outputs": [],
   "source": [
    "input_shape=(img_h, img_w, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4vItMmjoYBqk"
   },
   "outputs": [],
   "source": [
    "# network definition\n",
    "base_network = network_architecture(input_shape)\n",
    "input_a = Input(shape=(input_shape))\n",
    "input_b = Input(shape=(input_shape))\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "# Compute the Euclidean distance between the two vectors in the latent space\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "model = Model([input_a, input_b], distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qS0kJZhYBqn",
    "outputId": "18ffa467-c373-4fc9-f4fa-85f184ae5c0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 155, 220, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 155, 220, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 128)          6461084     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           sequential[0][0]                 \n",
      "                                                                 sequential[1][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,461,084\n",
      "Trainable params: 6,460,974\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFmTxSaCYBqq",
    "outputId": "908d9f55-7007-4887-e9be-cc12307ad03d"
   },
   "outputs": [],
   "source": [
    "base_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAHSmObIYBqu",
    "outputId": "b549b63b-0506-4b46-a48a-aa3183646f55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9660, 6360, 6360)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sz = 128\n",
    "num_train_samples = 66*70 + 72*70\n",
    "num_val_samples = num_test_samples = 66*20 +72*70 \n",
    "num_train_samples, num_val_samples, num_test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hB8nWS99YBqx"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fA6p0JaJYBqx"
   },
   "outputs": [],
   "source": [
    "# compile model using RMSProp Optimizer and Contrastive loss function defined above\n",
    "rms = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08)\n",
    "model.compile(loss=contrastive_loss, optimizer=rms, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSuVA1yyYBq0"
   },
   "outputs": [],
   "source": [
    "# Using Keras Callbacks, save the model after every epoch\n",
    "# Reduce the learning rate by a factor of 0.1 if the validation loss does not improve for 5 epochs\n",
    "# Stop the training using early stopping if the validation loss does not improve for 12 epochs\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=12, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000001, verbose=1),\n",
    "    ModelCheckpoint('/content/drive/My Drive/Colab Notebooks/weights3/signet-Engsig61-{epoch:03d}.h5', monitor='val_loss', verbose=1,save_best_only=True,save_weights_only=True,mode='min')\n",
    "] #260-{epoch:03d}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rwqbDpdOYBq7",
    "outputId": "0cef4811-0967-4395-8a68-53bcb5facadf"
   },
   "outputs": [],
   "source": [
    "results = model.fit(generate_batch(orig_train, forg_train, batch_sz),\n",
    "                              steps_per_epoch = num_train_samples//batch_sz,\n",
    "                              epochs = 20,\n",
    "                              validation_data = generate_batch(orig_val, forg_val, batch_sz),\n",
    "                              validation_steps = num_val_samples//batch_sz,\n",
    "                              callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuD8Bm23YBrB"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "U0Lna1HrYBrC"
   },
   "outputs": [],
   "source": [
    "tpr_list = []\n",
    "tnr_list = []\n",
    "threshold_list = []\n",
    "\n",
    "def compute_accuracy_roc(predictions, labels):\n",
    "    '''Compute accuracy with a range of thresholds on distances.\n",
    "    '''\n",
    "    dmax = np.max(predictions)\n",
    "    dmin = np.min(predictions)\n",
    "    nsame = np.sum(labels == 1)\n",
    "    ndiff = np.sum(labels == 0)\n",
    "   \n",
    "    step = 0.01\n",
    "    max_acc = 0\n",
    "    best_thresh = -1\n",
    "   \n",
    "    for d in np.arange(dmin, dmax+step, step):\n",
    "        idx1 = predictions.ravel() <= d\n",
    "        idx2 = predictions.ravel() > d\n",
    "       \n",
    "        tpr = float(np.sum(labels[idx1] == 1)) / nsame       \n",
    "        tnr = float(np.sum(labels[idx2] == 0)) / ndiff\n",
    "        acc = 0.5 * (tpr + tnr)       \n",
    "\n",
    "        if (acc > max_acc):\n",
    "            max_acc, best_thresh = acc, d\n",
    "        tpr_list.append(tpr)\n",
    "        tnr_list.append(tnr)\n",
    "        threshold_list.append(best_thresh)\n",
    "           \n",
    "    return max_acc, best_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfaic-ZdYBrF"
   },
   "source": [
    "### Loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JbRIgCNiYBrF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8865117043155311, 0.03012328206503298)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(path_weights + 'signet-Engsig61-006.h5')\n",
    "test_gen = generate_batch(orig_test, forg_test, 1)\n",
    "pred, tr_y = [], [] #tr_y have label\n",
    "for i in range(num_test_samples):\n",
    "    (img1, img2), label = next(test_gen)\n",
    "    tr_y.append(label)\n",
    "    pred.append(model.predict([img1, img2])[0][0])\n",
    "acc, threshold = compute_accuracy_roc(np.array(pred), np.array(tr_y))\n",
    "acc, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afRft2ltBsHg"
   },
   "source": [
    "### Precision Recall and F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qDxSvWtBmp6"
   },
   "outputs": [],
   "source": [
    "pred_binary = [1 if x<0.012809 else 0 for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClnAfP_UBrBo",
    "outputId": "433af058-baec-43eb-bcac-04ef5fcc8a4a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(pred_binary, tr_y,average='macro')\n",
    "precision,recall,fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-ffAl-JD6C9"
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "ndm1DDzVIulJ",
    "outputId": "64c6ed07-7767-459e-eac9-05c5e99c6229"
   },
   "outputs": [],
   "source": [
    "plt.plot(threshold_list,tpr_list,'g--')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('True Positive Rate')\n",
    "\n",
    "from google.colab import files\n",
    "plt.savefig(\"TPR.png\")\n",
    "files.download(\"TPR.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "lNyGzbarL2Er",
    "outputId": "4b1f5020-70e3-414c-e7d1-6817ead1890a"
   },
   "outputs": [],
   "source": [
    "plt.plot(threshold_list,tnr_list,'y--')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('True Negative Rate')\n",
    "\n",
    "from google.colab import files\n",
    "plt.savefig(\"TNR.png\")\n",
    "files.download(\"TNR.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "D2oZSehyJIBF",
    "outputId": "150e6cd8-1a21-4206-abfd-a55c69bec428"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(threshold_list, tnr_list, 'y--', label='TNR')\n",
    "ax.plot(threshold_list, tpr_list, 'g--', label='TPR')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(0.0128,tr_acc,'ro', label='EER') \n",
    "\n",
    "legend = ax.legend(loc='upper center', shadow=True, fontsize='x-large')\n",
    "# Put a nicer background color on the legend.\n",
    "legend.get_frame().set_facecolor('C0')\n",
    "\n",
    "from google.colab import files\n",
    "plt.savefig(\"EER.png\")\n",
    "files.download(\"EER.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Hyg_6fpYBrO"
   },
   "source": [
    "#### Max Accuracy = 92.56%(varies 1-2% due to random batch generation) and Threshold = 0.03012 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sv2znb-gYBrP"
   },
   "outputs": [],
   "source": [
    "def verify(img1, img2):\n",
    "    test_point, test_label = next(test_gen)\n",
    "    img1, img2 = test_point[0], test_point[1]\n",
    "    #print(img1)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 10))\n",
    "    ax1.imshow(np.squeeze(img1), cmap='gray')\n",
    "    ax2.imshow(np.squeeze(img2), cmap='gray')\n",
    "    ax1.set_title('Genuine')\n",
    "    if test_label == 1:\n",
    "        ax2.set_title('Genuine')\n",
    "    else:\n",
    "        ax2.set_title('Forged')\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    plt.show()\n",
    "    result = model.predict([img1, img2])\n",
    "    diff = result[0][0]\n",
    "    print(\"Difference Score = \", diff)\n",
    "    if diff > threshold:\n",
    "        print(\"Its a Forged Signature\")\n",
    "    else:\n",
    "        print(\"Its a Genuine Signature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi4OmSp4YBto"
   },
   "source": [
    "### Saving Model Architecture for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7t12iVjYBtp"
   },
   "outputs": [],
   "source": [
    "model_json=model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LK0Fhj23YBtr"
   },
   "outputs": [],
   "source": [
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nSS12X5YBts"
   },
   "outputs": [],
   "source": [
    "siamese_json=base_network.to_json()\n",
    "with open(\"siamese_BaseNetwork.json\",\"w\") as json_file:\n",
    "    json_file.write(siamese_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "m9KyTdHPNa7d",
    "outputId": "011d6386-b567-4949-ab67-70bd3048c887"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"model.json\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ebt6aR9pNghd",
    "outputId": "40aa721e-2548-438b-ca81-b57d07abfd7f"
   },
   "outputs": [],
   "source": [
    "files.download('siamese_BaseNetwork.json')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "name": "signature_verification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
